{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To carry out inference, please follow these steps, you don't actually need any API keys for inference, nor GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "from transformers import AutoModel, AutoTokenizer, AutoConfig\n",
    "\n",
    "hf_repo_id = \"Savoxism/phobert_v2.2_multilabel\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(hf_repo_id)\n",
    "\n",
    "class PhoBERTMultiAspectModel(nn.Module):\n",
    "    def __init__(self, model_name, num_aspects):\n",
    "        super(PhoBERTMultiAspectModel, self).__init__()\n",
    "\n",
    "        self.bert = AutoModel.from_pretrained(model_name, output_hidden_states=True)\n",
    "        self.config = AutoConfig.from_pretrained(model_name)  \n",
    "        self.hidden_size = self.bert.config.hidden_size * 4  # Concatenating last 4 layers\n",
    "\n",
    "        # Create a classifier for each aspect (4 logits per aspect)\n",
    "        self.aspect_classifiers = nn.ModuleList([\n",
    "            nn.Linear(self.hidden_size, 4) for _ in range(num_aspects)\n",
    "        ])\n",
    "\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, token_type_ids=None, labels=None):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask, \n",
    "                            token_type_ids=token_type_ids if token_type_ids is not None else None)\n",
    "        \n",
    "        # Get the hidden states from the BERT output\n",
    "        hidden_states = outputs.hidden_states \n",
    "        \n",
    "        # Concatenate last 4 hidden layers\n",
    "        pooled_output = torch.cat([hidden_states[-i][:, 0, :] for i in range(1, 5)], dim=-1)\n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "\n",
    "        # Apply classifiers to each aspect\n",
    "        aspect_outputs = [classifier(pooled_output) for classifier in self.aspect_classifiers]\n",
    "\n",
    "        return torch.stack(aspect_outputs, dim=1) # (batch_size, num_aspects, 4)\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = PhoBERTMultiAspectModel(model_name=\"vinai/phobert-base\", num_aspects=34)\n",
    "model.load_state_dict(torch.hub.load_state_dict_from_url(f\"https://huggingface.co/{hf_repo_id}/resolve/main/pytorch_model.bin\", map_location=device))\n",
    "model.to(device)\n",
    "model.eval()  \n",
    "\n",
    "# ✅ Define list of aspect categories\n",
    "label_keys = [\n",
    "    \"FACILITIES#CLEANLINESS\", \"FACILITIES#COMFORT\", \"FACILITIES#DESIGN&FEATURES\", \"FACILITIES#GENERAL\",\n",
    "    \"FACILITIES#MISCELLANEOUS\", \"FACILITIES#PRICES\", \"FACILITIES#QUALITY\", \"FOOD&DRINKS#MISCELLANEOUS\",\n",
    "    \"FOOD&DRINKS#PRICES\", \"FOOD&DRINKS#QUALITY\", \"FOOD&DRINKS#STYLE&OPTIONS\", \"HOTEL#CLEANLINESS\",\n",
    "    \"HOTEL#COMFORT\", \"HOTEL#DESIGN&FEATURES\", \"HOTEL#GENERAL\", \"HOTEL#MISCELLANEOUS\", \"HOTEL#PRICES\",\n",
    "    \"HOTEL#QUALITY\", \"LOCATION#GENERAL\", \"ROOMS#CLEANLINESS\", \"ROOMS#COMFORT\", \"ROOMS#DESIGN&FEATURES\",\n",
    "    \"ROOMS#GENERAL\", \"ROOMS#MISCELLANEOUS\", \"ROOMS#PRICES\", \"ROOMS#QUALITY\", \"ROOM_AMENITIES#CLEANLINESS\",\n",
    "    \"ROOM_AMENITIES#COMFORT\", \"ROOM_AMENITIES#DESIGN&FEATURES\", \"ROOM_AMENITIES#GENERAL\",\n",
    "    \"ROOM_AMENITIES#MISCELLANEOUS\", \"ROOM_AMENITIES#PRICES\", \"ROOM_AMENITIES#QUALITY\", \"SERVICE#GENERAL\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(text):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=256)\n",
    "    inputs = {key: value.to(device) for key, value in inputs.items()}  \n",
    "\n",
    "    # Run inference\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)  # Shape: (batch_size, num_aspects, 4)\n",
    "\n",
    "    # convert logits to class predictions\n",
    "    predicted_labels = torch.argmax(outputs, dim=-1).cpu().numpy()  # ✅ Ensure output is on CPU\n",
    "\n",
    "    # convert to list\n",
    "    predicted_labels = predicted_labels.flatten().tolist()  \n",
    "\n",
    "    # Sentiment mapping\n",
    "    sentiment_map = {0: \"None\", 1: \"Positive\", 2: \"Negative\", 3: \"Neutral\"}\n",
    "    \n",
    "    aspect_predictions = {aspect: sentiment_map[int(pred)] for aspect, pred in zip(label_keys, predicted_labels)}\n",
    "    return aspect_predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_text = input()\n",
    "predictions = predict(example_text)\n",
    "\n",
    "predictions_df = pd.DataFrame(list(predictions.items()), columns=[\"Aspect\", \"Sentiment\"])\n",
    "print(predictions_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
