{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "%%capture\n",
    "!pip install -q datasets\n",
    "!pip install -q transformers\n",
    "!pip install -q emoji\n",
    "!pip install -q vncorenlp\n",
    "!git clone https://github.com/vncorenlp/VnCoreNLP.git\n",
    "!pip install wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!wget https://github.com/Savoxism/ABSA-Vietnamese/archive/refs/heads/main.zip -O ABSA-Vietnamese.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!unzip ABSA-Vietnamese.zip -d /kaggle/working/\n",
    "!ls /kaggle/working/ABSA-Vietnamese-main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd /kaggle/working/ABSA-Vietnamese-main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import flatten\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from datasets import load_dataset\n",
    "from vncorenlp import VnCoreNLP\n",
    "\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from huggingface_hub import login\n",
    "import wandb\n",
    "from transformers import AutoTokenizer, AutoModel, AutoConfig, Trainer, TrainingArguments\n",
    "\n",
    "from preprocess import (\n",
    "    remove_HTML, \n",
    "    convert_unicode, \n",
    "    standardize_sentence_typing, \n",
    "    normalize_acronym, \n",
    "    remove_unnecessary_characters,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ANNOTATOR_PATH = \"/kaggle/working/VnCoreNLP/VnCoreNLP-1.1.1.jar\"  # Change to correct path\n",
    "\n",
    "if not os.path.exists(ANNOTATOR_PATH):\n",
    "    raise FileNotFoundError(f\"VnCoreNLP JAR file not found at {ANNOTATOR_PATH}\")\n",
    "\n",
    "annotator = VnCoreNLP(ANNOTATOR_PATH)\n",
    "\n",
    "def word_segmentation(text):\n",
    "    words = annotator.tokenize(text)\n",
    "    return ' '.join(word for word in flatten(words))\n",
    "\n",
    "def text_preprocess(text):\n",
    "    text = remove_HTML(text)\n",
    "    text = convert_unicode(text) \n",
    "    text = standardize_sentence_typing(text)\n",
    "    text = normalize_acronym(text)\n",
    "    text = word_segmentation(text) \n",
    "    text = remove_unnecessary_characters(text)\n",
    "    # return text.lower()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    print(\"GPU is available!\")\n",
    "    print(\"Device:\", torch.cuda.get_device_name(0))\n",
    "else:\n",
    "    print(\"GPU not available. Please enable GPU in Kaggle settings.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kaggle_secrets import UserSecretsClient\n",
    "user_secrets = UserSecretsClient()\n",
    "hf_token = user_secrets.get_secret(\"HF_AUTH_TOKEN\")\n",
    "login(token=hf_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wb_token = user_secrets.get_secret(\"wandb_api_key\")\n",
    "\n",
    "wandb.login(key=wb_token)\n",
    "run = wandb.init(\n",
    "    project='ABSA-Vietnamese', \n",
    "    job_type=\"training\", \n",
    "    anonymous=\"allow\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_PATH = \"/kaggle/input/vlsp2018-hotel/1-VLSP2018-SA-Hotel-train.csv\"\n",
    "VAL_PATH = \"/kaggle/input/vlsp2018-hotel/2-VLSP2018-SA-Hotel-dev.csv\"\n",
    "TEST_PATH = \"/kaggle/input/vlsp2018-hotel/3-VLSP2018-SA-Hotel-test.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_datasets = load_dataset('csv', data_files={'train': TRAIN_PATH, 'val': VAL_PATH, 'test': TEST_PATH})\n",
    "raw_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PRETRAINED_MODEL = 'vinai/phobert-base'\n",
    "tokenizer = AutoTokenizer.from_pretrained(PRETRAINED_MODEL)\n",
    "print(\"Max Model Input Size:\", tokenizer.model_max_length)\n",
    "print(tokenizer.model_input_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokens = tokenizer.encode('T√¥i l√† sinh vi√™n tr∆∞·ªùng ƒë·∫°i h·ªçc C√¥ng ngh·ªá th√¥ng tin .') \n",
    "tokens = tokenizer.encode('T√¥i l√† sinh_vi√™n tr∆∞·ªùng ƒë·∫°i_h·ªçc C√¥ng_ngh·ªá th√¥ng_tin .') # When use PhoBERT\n",
    "print('Encode:', tokens)\n",
    "print('Decode:', tokenizer.decode(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_keys = [\n",
    "    key for key in raw_datasets['train'].column_names  \n",
    "    if key not in [\"Review\", \"input_ids\", \"token_type_ids\", \"attention_mask\"]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(label_keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_sentiment(sentiment):\n",
    "    sentiment_map = {\n",
    "        0: [1, 0, 0, 0],  # None\n",
    "        1: [0, 1, 0, 0],  # Positive\n",
    "        2: [0, 0, 1, 0],  # Negative\n",
    "        3: [0, 0, 0, 1]   # Neutral\n",
    "    }\n",
    "    return sentiment_map.get(sentiment, [1, 0, 0, 0]) \n",
    "\n",
    "def clean_text(review):\n",
    "    # print(\"used\")\n",
    "    return text_preprocess(review) if isinstance(review, str) else \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example = raw_datasets[\"train\"][0]\n",
    "example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Clean text\n",
    "cleaned_text_result = clean_text(example[\"Review\"])\n",
    "print(\"Cleaned Text:\", cleaned_text_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_review_text(example):\n",
    "    example[\"Review\"] = clean_text(example[\"Review\"])\n",
    "    return example\n",
    "\n",
    "# Apply the cleaning function to all dataset splits (train, val, test)\n",
    "cleaned_datasets = raw_datasets.map(clean_review_text, batched=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_datasets['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_cleaned_example(example):\n",
    "    \"\"\"Tokenizes the cleaned text and encodes sentiment labels.\"\"\"\n",
    "\n",
    "    # Step 1: Tokenize the input text\n",
    "    tokenized_inputs = tokenizer(\n",
    "        example[\"Review\"],  # Already cleaned text\n",
    "        max_length=256, \n",
    "        padding=\"max_length\", \n",
    "        truncation=True\n",
    "    )\n",
    "\n",
    "    aspect_labels = [encode_sentiment(example.get(key, 0)) for key in label_keys]\n",
    "    tokenized_inputs[\"labels\"] = np.array(aspect_labels, dtype=np.int64)  # Shape: (34, 4)\n",
    "\n",
    "    return tokenized_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply tokenization to the cleaned dataset\n",
    "tokenized_dataset = cleaned_datasets.map(tokenize_cleaned_example, batched=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_dataset['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select only necessary columns\n",
    "columns_to_keep = [\"input_ids\", \"attention_mask\", \"labels\"]\n",
    "tokenized_dataset = tokenized_dataset.remove_columns(\n",
    "    [col for col in tokenized_dataset[\"train\"].column_names if col not in columns_to_keep]\n",
    ")\n",
    "\n",
    "# Verify the new dataset structure\n",
    "print(tokenized_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_dataset['train'][0]['labels']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example = tokenized_dataset[\"train\"][0] \n",
    "print(\"Keys in example:\", example.keys())  # Should include 'labels'\n",
    "print(\"Input IDs:\", example[\"input_ids\"][:10])  \n",
    "print(\"Labels shape:\", len(example[\"labels\"]), \"x\", len(example[\"labels\"][0]))  # Should be (34, 4)\n",
    "print(\"Labels:\", example[\"labels\"])  # Should be a list of 34 lists, each with 4 elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    input_ids = torch.tensor([item[\"input_ids\"] for item in batch], dtype=torch.long)\n",
    "    attention_mask = torch.tensor([item[\"attention_mask\"] for item in batch], dtype=torch.long)\n",
    "\n",
    "    # Convert labels into a tensor with correct shape (batch_size, 34, 4)\n",
    "    labels = torch.tensor([item[\"labels\"] for item in batch], dtype=torch.float)  \n",
    "\n",
    "    batch_dict = {\n",
    "        \"input_ids\": input_ids,         # Shape: (batch_size, seq_length)\n",
    "        \"attention_mask\": attention_mask,  # Shape: (batch_size, seq_length)\n",
    "        \"labels\": labels  # Shape: (batch_size, 34, 4)\n",
    "    }\n",
    "\n",
    "    return batch_dict\n",
    "\n",
    "# Create DataLoaders for training and evaluation\n",
    "train_loader = DataLoader(tokenized_dataset[\"train\"], batch_size=8, collate_fn=collate_fn, shuffle=True)\n",
    "eval_loader = DataLoader(tokenized_dataset[\"val\"], batch_size=8, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(train_loader))\n",
    "\n",
    "print(\"Batch keys:\", batch.keys())  # Should include 'labels'\n",
    "print(\"Input IDs shape:\", batch[\"input_ids\"].shape)  # Expected (batch_size, sequence_length)\n",
    "print(\"Labels shape:\", batch[\"labels\"].shape)  # Expected (batch_size, 34, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PhoBERTMultiAspectModel(nn.Module):\n",
    "   def __init__(self, model_name, num_aspects):\n",
    "        super(PhoBERTMultiAspectModel, self).__init__()\n",
    "\n",
    "        self.bert = AutoModel.from_pretrained(model_name, output_hidden_states=True)\n",
    "        self.config = AutoConfig.from_pretrained(model_name)  \n",
    "        self.hidden_size = self.bert.config.hidden_size * 4 \n",
    "\n",
    "        # Create a classifier for each aspect (4 logits per aspect)\n",
    "        self.aspect_classifiers = nn.ModuleList([\n",
    "            nn.Linear(self.hidden_size, 4) for _ in range(num_aspects)\n",
    "        ])\n",
    "\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "\n",
    "   def forward(self, input_ids, attention_mask, token_type_ids=None, labels=None):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask, \n",
    "                            token_type_ids=token_type_ids if token_type_ids is not None else None)\n",
    "        \n",
    "        hidden_states = outputs.hidden_states  # Shape: (batch_size, seq_len, hidden_dim)\n",
    "\n",
    "        # Correctly concatenate the last 4 hidden layers\n",
    "        pooled_output = torch.cat([hidden_states[-i][:, 0, :] for i in range(1, 5)], dim=-1)\n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "\n",
    "        # Apply classifiers to each aspect\n",
    "        aspect_outputs = [classifier(pooled_output) for classifier in self.aspect_classifiers]\n",
    "\n",
    "        # Return a structured output: (batch_size, num_aspects, 4)\n",
    "        return torch.stack(aspect_outputs, dim=1)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PhoBERTTrainer(Trainer):\n",
    "    def get_train_dataloader(self):\n",
    "        print(\"‚úÖ Using custom train DataLoader\")\n",
    "        return train_loader  # ‚úÖ Ensures labels are included in batches\n",
    "\n",
    "    def get_eval_dataloader(self, eval_dataset=None):\n",
    "        \"\"\"Forces Trainer to use the custom eval DataLoader.\"\"\"\n",
    "        print(\"‚úÖ Using custom eval DataLoader\")  \n",
    "        return eval_loader  # ‚úÖ Ensures validation labels are included\n",
    "\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
    "        \"\"\"Custom loss function to correctly process multi-aspect labels.\"\"\"\n",
    "        # print(\"Keys in inputs:\", inputs.keys())  # Debugging print\n",
    "\n",
    "        if \"labels\" not in inputs:\n",
    "            raise ValueError(f\"üö® Missing 'labels' in inputs. Available keys: {inputs.keys()}\")\n",
    "\n",
    "        labels = inputs[\"labels\"]  \n",
    "        outputs = model(**inputs)  # Forward pass ‚Üí (batch_size, num_aspects, 4)\n",
    "\n",
    "        # Convert one-hot encoded labels to class indices\n",
    "        labels = torch.argmax(labels, dim=-1)  # Shape: (batch_size, num_aspects)\n",
    "\n",
    "        # Compute loss\n",
    "        loss_fn = nn.CrossEntropyLoss()\n",
    "        loss = loss_fn(outputs.view(-1, 4), labels.view(-1)) \n",
    "\n",
    "        return (loss, outputs) if return_outputs else loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PhoBERTMultiAspectModel(\"vinai/phobert-base\", len(label_keys))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./phobert_multilabel-V2\",\n",
    "    eval_strategy=\"no\",\n",
    "    save_strategy=\"no\",\n",
    "    load_best_model_at_end=False,\n",
    "    num_train_epochs=10,  # Adjust based on dataset size\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=200,\n",
    "    save_total_limit=2,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    report_to=\"wandb\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = PhoBERTTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=None,  # No need for dataset since we use DataLoader\n",
    "    eval_dataset=None,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "# Run training\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = trainer.evaluate()\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_history = trainer.state.log_history\n",
    "df = pd.DataFrame(log_history)\n",
    "df_loss = df[df['loss'].notna()]\n",
    "\n",
    "# Plot the training loss over time\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(df_loss['step'], df_loss['loss'], label=\"Training Loss\", color=\"blue\")\n",
    "plt.xlabel(\"Training Steps\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"PhoBERT Training Loss Over Time\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "\n",
    "# Define save path\n",
    "model_save_path = \"./phobert_multilabel-V2\"\n",
    "\n",
    "# Ensure the save directory exists\n",
    "os.makedirs(model_save_path, exist_ok=True)\n",
    "\n",
    "# ‚úÖ Force CPU mode while saving\n",
    "torch.save(model.cpu().state_dict(), f\"{model_save_path}/pytorch_model.bin\")\n",
    "\n",
    "# Save tokenizer\n",
    "tokenizer.save_pretrained(model_save_path)\n",
    "\n",
    "# Save model config (useful when loading later)\n",
    "model.config.to_json_file(f\"{model_save_path}/config.json\")\n",
    "\n",
    "# Confirm saving is successful\n",
    "print(f\"‚úÖ Model and tokenizer saved to {model_save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!zip -r phobert-v2-multilabel.zip /kaggle/working/ABSA-Vietnamese-main/phobert_multilabel-V2"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
